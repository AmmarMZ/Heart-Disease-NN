{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import sys\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data taken from https://www.kaggle.com/ronitf/heart-disease-uci#heart.csv\n",
    "datafile = \"data/heart.csv\"\n",
    "\n",
    "data = np.zeros((303,14))\n",
    "\n",
    "# reading data from csv and putting into np array\n",
    "with open(datafile) as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    i = 0\n",
    "    count = 0\n",
    "    for row in csvReader:\n",
    "        row = np.reshape(row,len(row),1)\n",
    "        if (count == 0):\n",
    "            count = -1\n",
    "            continue\n",
    "        else:\n",
    "            data[i,:] = row\n",
    "            i += 1\n",
    "\n",
    "# X.shape = 303 x 13\n",
    "Xt = data[:,0:13]\n",
    "\n",
    "# y.shape = 303 x 1\n",
    "yT = data[:,13]\n",
    "yT = yT.reshape(len(yT),1)\n",
    "\n",
    "combined = np.append(Xt, yT, axis = 1)\n",
    "np.random.shuffle(combined)\n",
    " \n",
    "# X and y have been randomized by the same factor\n",
    "X = combined[:,:13]\n",
    "y = combined[:,13]\n",
    "y = y.reshape(len(y),1)\n",
    "\n",
    "XOG = torch.tensor(Xt, dtype = torch.float)\n",
    "yOG = torch.tensor(yT, dtype = torch.float)\n",
    "\n",
    "\n",
    "y = torch.tensor(y, dtype = torch.float)\n",
    "X = torch.tensor(X, dtype = torch.float)\n",
    "\n",
    "XTrain = X[:212,:]\n",
    "XTest = X[212:,:]\n",
    "\n",
    "yTrain = y[:212,:]\n",
    "yTest = y[212:,:]\n",
    "\n",
    "# 303\n",
    "m = X.shape[0]\n",
    "mTest = xTest.shape[0]\n",
    "mTrain = xTrain.shape[0]\n",
    "\n",
    "#13\n",
    "n = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-03ada692c8f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mlossFunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[0;32m     40\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[0;32m     41\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "inputLayerSize = 13\n",
    "hiddenLayerSize1 = 7\n",
    "hiddenLayerSize2 = 7\n",
    "outputLayerSize = 1\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#model = torch.nn.Sequential(\n",
    "#        torch.nn.Linear(inputLayerSize, hiddenLayerSize1),\n",
    "#        torch.nn.ReLU(),\n",
    "#        torch.nn.Linear(hiddenLayerSize1, hiddenLayerSize2),\n",
    "#        torch.nn.ReLU(),\n",
    "#        torch.nn.Linear(hiddenLayerSize2, outputLayerSize)\n",
    "#).to(device)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __int__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(inputLayerSize,hiddenLayerSize1)\n",
    "        self.layer2 = torch.nn.Linear(hiddenLayerSize1, hiddenLayerSize2)\n",
    "        self.layer3 = torch.nn.Linear(hiddenLayerSize2, outputLayerSize)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a2 = F.sigmoid(self.layer1(x))\n",
    "        a3 = F.sigmoid(a2)\n",
    "        output = self.layer3(a3)\n",
    "        return output\n",
    "\n",
    "    \n",
    "model = Model()\n",
    "model.parameters()\n",
    "print(model)\n",
    "\n",
    "\n",
    "lossFunc = torch.nn.MSELoss(reduction = 'sum')\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "def train():\n",
    "    for i in range(m):\n",
    "        yPred = model(X[i])\n",
    "        loss = lossFunc(yPred, y[i])\n",
    "                      \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "                                            \n",
    "        if (i == mTrain - 1):\n",
    "            print(\"Loss value: \" + str(loss.item()))\n",
    "\n",
    "start = time.time()\n",
    "for i in range(30):\n",
    "    sys.stdout.write(\"Iteration %d \"%i)\n",
    "    sys.stdout.flush()\n",
    "    train()\n",
    "end = time.time()\n",
    "print(\"Training Completed- Total time is %f seconds\" %(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.181518151815181\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(mTest):\n",
    "    val = model(xTest[i])\n",
    "    #print(str(val.item()) + \" \" + str(y[i].item()))\n",
    "    if (abs(yTest[i].item() - val.item()) <= 0.5):\n",
    "        count += 1\n",
    "        \n",
    "print(count/m * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
